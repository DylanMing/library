---
tags:
  - AI八股
  - AI基础
  - 机器学习
  - 评估指标


---
[^1]  [维基百科-混淆矩阵](https://en.wikipedia.org/wiki/Confusion_matrix#cite_note-11)
[^2] [分类模型评判指标--混淆矩阵](https://zhuanlan.zhihu.com/p/111274912)
[^3]  [一文彻底理解 ROC/AUC 概念（Python）](https://mp.weixin.qq.com/s/sT2Y_kMetS91iUPH7XUgqg)
[^4]  [维基百科-ROC曲线](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)


对机器学习模型的性能，我们需要进行评估，所以就有了评估指标

# 混淆矩阵，准确率，精准率，召回率

## 混淆矩阵

**混淆矩阵 confusion matrix**，常用于机器学习统计学习等**分类问题**中，特别用于监督学习，无监督学习中一般叫做 **匹配矩阵 matching matrix**,他的作用就是可以很方便地看出分类器是否把两个不同的类混淆了。

混淆矩阵中有四个主要数字，他们属于一级指标：

- **真阳性 TP True Positive** : 分类正确，即预测标签为真值，真实标签为真值
- **假阴性 FN False Nagative** ：第一类错误，预测标签为假值，真实标签为真值
- **假阳性 FP False Positive**： 第二类错误，预测标签为真值，真实标签为假值
- **真阴性 TN True Nagative** ：分类正确，即预测标签为假值，真实标签为假值

即按照 **分类结果**（*正确/错误，True/False*）和 **分类类别**（*真值/假值， Positive/Negative*）进行划分。

根据这四个数字我们可以画出混淆矩阵：

| 总数=P+N | 预测真值  | 预测假值  |
| -------- | --------- | --------- |
| 实际真值 | 真阳性 TP | 假阴性 FN |
| 实际假值 | 假阳性 FP | 真阴性 TN |

对于分类器的预测性能，当然是希望预测得越准确越好,那么就是真阳性TP和真阴性TN得的数量越多越好

## 准确率，精准率，召回率

而对于大量数据，通过数量很难衡量模型的优劣，使用比率进行表示，出现了四个二级指标

- **准确率 ACC Accuracy** : 所有预测值中，分类正确的占比,
  $$
  \text{ACCuracy}=\frac{TP+TN}{P+N}=\frac{TP+TN}{TP+FP+TN+FN}
  $$
- **精确率 precision** /**真测率 positive predictive value (PPV)**：预测为真值的结果中，正确分类为真值的占比
  $$
  \text{Precision/PPV}=\frac{TP}{TP+FP}
  $$
- **召回率 Recall** /**敏感度 sensitivity**/**真阳性率 true positive rate (TPR)**: 真实值为真值的结果中，正确分类为真值的占比
  $$
  \text{Recall}=\text{TPR}=\frac{TP}{TP+FN}
  $$
- **特异度 Specificity**/ **真负率 true negative rate (TNR)** ： 真实值为假值的结果中，正确分类为假值的占比
  $$
  \text{Specificity/TNR}=\frac{TN}{N}=\frac{TN}{TN+FP}
  $$

还有一些如假阴率，假阳率等，在机器学习中不常用，可参考维基百科。

# F1值

四个二级指标可以对模型效果进行标准化衡量，某种意义上是对结果的正则化，而怎样综合这些不同的指标，给出模型的综合得分呢？

可以由**F1 Score**来表示, 他综合了**精确率PPV**和**召回率Recall** 的效果

$$
F_1=\frac{2}{\frac{1}{PPV}+\frac{1}{TPR}}=\frac{2}{\frac{1}{精确率}+\frac{1}{召回率}}=2\frac{PPV\times TPR}{PPV+TPR}=\frac{TP}{TP+\frac{FN+FP}{2}}
$$

可以由精确率，召回率 两者积除以两者和 的二倍来计算，最后可以化简成由一级指标表示的形式

可以看出F1分数取值范围为0-1，越靠近1越好，即模型全部预测正确，0表示模型效果最差，

# ROC和AUC

## ROC曲线

**ROC曲线**， **receiver operating characteristic curve**，操作者接收曲线，来自于信号检测理论，首先由二战中的电子工程师和雷达工程师发明，用于侦测战场上的敌军载具。后来用于各种统计学和机器学习中。

ROC空间将 **伪阳性率(FPR)** 定义为X轴，**真阳性率(TPR)** 定义为Y轴

真阳性率其实就是召回率，表示所有实际为阳性的结果中，正确预测为阳性的占比，
伪阳性率可以类比，表示所有实际为阴性的结果中，错误预测为阳性的占比

$$
TPR=Recall=\frac{TP}{TP+FN}
$$

$$
FPR=\frac{FP}{FP+TN}
$$

完美的预测是在左上角的点，在ROC空间中坐标为(0,1) ，X=0表示没有伪阳性，Y=1表示没有伪阴性（所有的阳性都是真阳性），也就是说，不管分类器输出是阳性或阴性，都是正确的。一个随机的预测会得到位于从(0,0)到(1,1)的对角线上一点。

![[Pasted image 20230928033028.png|500]]

在分类器分类时，通常有一个**连续随机变量X**，是用来分类实例的分类分数，**类似分类的置信度或者概率**。如果模型计算出实例的该分数低于某个值X，那么就会将其分类为真值，高于某个值X，那么就会将其分类为假值，这个值X称为**阈值**
类似去医院验血，白细胞高于多少才算不正常，这个阈值就是一个标准。

将一个分类器的阈值设置为高或者低，可以得出不同的FPR和TPR，将同一个模型的每一个阈值的(FPR,TPR)都画在ROC空间中，就得到模型的ROC曲线。

ROC曲线越靠右上，则分类效果越好。

## AUC 曲线下面积

ROC曲线下方的面积（英语：Area Under the  ROC (AUROC/AUC)），其意义是：

![[Pasted image 20230928034528.png]]

- 假设阈值以上是阳性，以下是阴性；若随机抽取一个阳性样本和一个阴性样本，分类器**正确判断**阳性样本的值高于阴性样本之**概率** =AUC
- 因为是在1x1的方格里求面积，AUC必在0~1之间
- 简单说：**AUC值越大的分类器，正确率越高。**

从AUC判断分类器（预测模型）优劣的标准：

- AUC = 1，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器。
- 0.5 < AUC < 1，优于随机猜测。这个分类器（模型）妥善设置阈值的话，能有预测价值。
- AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。
- AUC < 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。

# Dice metric

Dice指数，又名Sørensen–Dice coefficient或者Dice similarity coefficient， 常用于医学图像分割领域，取值范围为0~1，1表示完全相同，0表示没有相似性，其计算如下：

$$
\text{Dice}=\frac{2\times(pred\cap true)}{pred\cup true}
$$

其中，`pred`代表预测值的集合，`true`代表真实值的集合。分子 `(pred ∩ true)`表示预测集合和真实集合的交集，即两个集合中相同的元素数量。分母 `(pred ∪ true)`表示预测集合和真实集合的并集，即两个集合中所有元素的数量。乘以2是为了校正因重复计算的交集部分。

Dice系数与机器学习中的F1分数（F1 Score）在数学上是等价的。F1分数是精确率（Precision）和召回率（Recall）的调和平均值，而Dice系数可以看作是在二分类问题中同时考虑了精确率和召回率的一个指标。因此，Dice系数也可以用来评估分类任务的性能。
